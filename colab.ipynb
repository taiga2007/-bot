{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F7aJhsgLAWvO"
      },
      "source": [
        "# Style-Bert-VITS2 (ver 2.7.0) のGoogle Colabでの学習\n",
        "\n",
        "Google Colab上でStyle-Bert-VITS2の学習を行うことができます。\n",
        "\n",
        "このnotebookでは、通常使用ではあなたのGoogle Driveにフォルダ`Style-Bert-VITS2`を作り、その内部での作業を行います。他のフォルダには触れません。\n",
        "Google Driveを使わない場合は、初期設定のところで適切なパスを指定してください。\n",
        "\n",
        "## 流れ\n",
        "\n",
        "### 学習を最初からやりたいとき\n",
        "上から順に実行していけばいいです。音声合成に必要なファイルはGoogle Driveの`Style-Bert-VITS2/model_assets/`に保存されます。また、途中経過も`Style-Bert-VITS2/Data/`に保存されるので、学習を中断したり、途中から再開することもできます。\n",
        "\n",
        "### 学習を途中から再開したいとき\n",
        "0と1を行い、3の前処理は飛ばして、4から始めてください。スタイル分け5は、学習が終わったら必要なら行ってください。\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L-gAIubBAWvQ"
      },
      "source": [
        "## 0. 環境構築\n",
        "\n",
        "Style-Bert-VITS2の環境をcolab上に構築します。ランタイムがT4等のGPUバックエンドになっていることを確認し、実行してください。\n",
        "\n",
        "**注意**: このセルを実行した後に「セッションがクラッシュしました」「不明な理由により、セッションがクラッシュしました。」等の警告が出ますが、**無視してそのまま先へ**進んでください。（一度ランタイムを再起動させてnumpy<2を強制させるため `exit()` を呼んでいることからの措置です。）"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0GNj8JyDAlm2",
        "outputId": "b3b110a2-762b-40dc-a3f2-4865ebf89a22"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "downloading uv 0.8.14 x86_64-unknown-linux-gnu\n",
            "no checksums to verify\n",
            "installing to /usr/local/bin\n",
            "  uv\n",
            "  uvx\n",
            "everything's installed!\n",
            "Cloning into 'Style-Bert-VITS2'...\n",
            "remote: Enumerating objects: 6852, done.\u001b[K\n",
            "remote: Counting objects: 100% (3817/3817), done.\u001b[K\n",
            "remote: Compressing objects: 100% (1066/1066), done.\u001b[K\n",
            "remote: Total 6852 (delta 2909), reused 2771 (delta 2751), pack-reused 3035 (from 1)\u001b[K\n",
            "Receiving objects: 100% (6852/6852), 14.58 MiB | 22.77 MiB/s, done.\n",
            "Resolving deltas: 100% (4605/4605), done.\n",
            "/content/Style-Bert-VITS2/Style-Bert-VITS2\n",
            "\u001b[2mUsing Python 3.12.11 environment at: /usr\u001b[0m\n",
            "\u001b[2mAudited \u001b[1m26 packages\u001b[0m \u001b[2min 300ms\u001b[0m\u001b[0m\n",
            "08-31 07:15:20 |  INFO  | initialize.py:19 | Downloading deberta-v2-large-japanese-char-wwm pytorch_model.bin\n",
            "pytorch_model.bin: 100% 1.32G/1.32G [00:07<00:00, 169MB/s]\n",
            "08-31 07:15:28 |  INFO  | initialize.py:19 | Downloading deberta-v2-large-japanese-char-wwm-onnx model_fp16.onnx\n",
            "model_fp16.onnx: 100% 653M/653M [00:05<00:00, 129MB/s]\n",
            "08-31 07:15:33 |  INFO  | initialize.py:19 | Downloading chinese-roberta-wwm-ext-large pytorch_model.bin\n",
            "pytorch_model.bin: 100% 1.31G/1.31G [00:11<00:00, 117MB/s]\n",
            "08-31 07:15:44 |  INFO  | initialize.py:19 | Downloading chinese-roberta-wwm-ext-large-onnx model_fp16.onnx\n",
            "model_fp16.onnx: 100% 599M/599M [00:04<00:00, 133MB/s]\n",
            "08-31 07:15:49 |  INFO  | initialize.py:19 | Downloading deberta-v3-large spm.model\n",
            "spm.model: 100% 2.46M/2.46M [00:00<00:00, 12.7MB/s]\n",
            "08-31 07:15:49 |  INFO  | initialize.py:19 | Downloading deberta-v3-large pytorch_model.bin\n",
            "pytorch_model.bin: 100% 874M/874M [00:09<00:00, 93.1MB/s]\n",
            "08-31 07:15:59 |  INFO  | initialize.py:19 | Downloading deberta-v3-large-onnx spm.model\n",
            "spm.model: 100% 2.46M/2.46M [00:00<00:00, 12.3MB/s]\n",
            "08-31 07:16:00 |  INFO  | initialize.py:19 | Downloading deberta-v3-large-onnx model_fp16.onnx\n",
            "model_fp16.onnx: 100% 864M/864M [00:09<00:00, 95.1MB/s]\n",
            "08-31 07:16:09 |  INFO  | initialize.py:27 | Downloading wavlm-base-plus pytorch_model.bin\n",
            "pytorch_model.bin: 100% 378M/378M [00:03<00:00, 122MB/s]\n",
            "08-31 07:16:12 |  INFO  | initialize.py:36 | Downloading pretrained G_0.safetensors\n",
            "G_0.safetensors: 100% 234M/234M [00:02<00:00, 106MB/s] \n",
            "08-31 07:16:14 |  INFO  | initialize.py:36 | Downloading pretrained D_0.safetensors\n",
            "D_0.safetensors: 100% 187M/187M [00:01<00:00, 118MB/s]\n",
            "08-31 07:16:16 |  INFO  | initialize.py:36 | Downloading pretrained DUR_0.safetensors\n",
            "DUR_0.safetensors: 100% 2.42M/2.42M [00:00<00:00, 6.61MB/s]\n",
            "08-31 07:16:17 |  INFO  | initialize.py:47 | Downloading JP-Extra pretrained G_0.safetensors\n",
            "G_0.safetensors: 100% 293M/293M [00:02<00:00, 130MB/s]\n",
            "08-31 07:16:19 |  INFO  | initialize.py:47 | Downloading JP-Extra pretrained D_0.safetensors\n",
            "D_0.safetensors: 100% 187M/187M [00:01<00:00, 126MB/s]\n",
            "08-31 07:16:21 |  INFO  | initialize.py:47 | Downloading JP-Extra pretrained WD_0.safetensors\n",
            "WD_0.safetensors: 100% 4.70M/4.70M [00:00<00:00, 25.1MB/s]\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "\n",
        "os.environ[\"PATH\"] += \":/root/.cargo/bin\"\n",
        "\n",
        "!curl -LsSf https://astral.sh/uv/install.sh | sh\n",
        "!git clone https://github.com/litagin02/Style-Bert-VITS2.git\n",
        "%cd Style-Bert-VITS2/\n",
        "!uv pip install --system -r requirements-colab.txt --no-progress\n",
        "!python initialize.py --skip_default_models\n",
        "\n",
        "exit()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 321
        },
        "id": "o5z1nzkvAWvR",
        "outputId": "9e5e3def-4c19-419f-d458-b5ca94524ce9"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "Mountpoint must not already contain files",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-805283205.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mgoogle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolab\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdrive\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mdrive\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"/content/drive\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/google/colab/drive.py\u001b[0m in \u001b[0;36mmount\u001b[0;34m(mountpoint, force_remount, timeout_ms, readonly)\u001b[0m\n\u001b[1;32m     95\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mmount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmountpoint\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mforce_remount\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout_ms\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m120000\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreadonly\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     96\u001b[0m   \u001b[0;34m\"\"\"Mount your Google Drive at the specified mountpoint path.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 97\u001b[0;31m   return _mount(\n\u001b[0m\u001b[1;32m     98\u001b[0m       \u001b[0mmountpoint\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     99\u001b[0m       \u001b[0mforce_remount\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mforce_remount\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/google/colab/drive.py\u001b[0m in \u001b[0;36m_mount\u001b[0;34m(mountpoint, force_remount, timeout_ms, ephemeral, readonly)\u001b[0m\n\u001b[1;32m    193\u001b[0m       \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Mountpoint must not be a symlink'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    194\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0m_os\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmountpoint\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0m_os\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlistdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmountpoint\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 195\u001b[0;31m       \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Mountpoint must not already contain files'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    196\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0m_os\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmountpoint\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0m_os\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexists\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmountpoint\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    197\u001b[0m       \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Mountpoint must either be a directory or not exist'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: Mountpoint must not already contain files"
          ]
        }
      ],
      "source": [
        "# Google driveを使う方はこちらを実行してください。\n",
        "\n",
        "from google.colab import drive\n",
        "\n",
        "drive.mount(\"/content/drive\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WU9apXzcAWvR"
      },
      "source": [
        "## 1. 初期設定\n",
        "\n",
        "学習とその結果を保存するディレクトリ名を指定します。\n",
        "Google driveの場合はそのまま実行、カスタマイズしたい方は変更して実行してください。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "gO3OwZV1AWvR",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "99d4b59e-31e5-4ed4-bb0d-e7f573722c98"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/Style-Bert-VITS2\n"
          ]
        }
      ],
      "source": [
        "# 作業ディレクトリを移動\n",
        "%cd /content/Style-Bert-VITS2/\n",
        "\n",
        "# 学習に必要なファイルや途中経過が保存されるディレクトリ\n",
        "dataset_root = \"/content/drive/MyDrive/Style-Bert-VITS2/Data\"\n",
        "\n",
        "# 学習結果（音声合成に必要なファイルたち）が保存されるディレクトリ\n",
        "assets_root = \"/content/drive/MyDrive/Style-Bert-VITS2/model_assets\"\n",
        "\n",
        "import yaml\n",
        "\n",
        "\n",
        "with open(\"configs/paths.yml\", \"w\", encoding=\"utf-8\") as f:\n",
        "    yaml.dump({\"dataset_root\": dataset_root, \"assets_root\": assets_root}, f)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dA_yLeezAWvS"
      },
      "source": [
        "## 2. 学習に使うデータ準備\n",
        "\n",
        "すでに音声ファイル（1ファイル2-12秒程度）とその書き起こしデータがある場合は2.2を、ない場合は2.1を実行してください。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8s9gOnTCAWvS"
      },
      "source": [
        "### 2.1 音声ファイルからのデータセットの作成（ある人はスキップ可）\n",
        "\n",
        "音声ファイル（1ファイル2-12秒程度）とその書き起こしのデータセットを持っていない方は、（日本語の）音声ファイルのみから以下の手順でデータセットを作成することができます。Google drive上の`Style-Bert-VITS2/inputs/`フォルダに音声ファイル（wavやmp3等の通常の音声ファイル形式、1ファイルでも複数ファイルでも可）を置いて、下を実行すると、データセットが作られ、自動的に正しい場所へ配置されます。\n",
        "\n",
        "**2024-06-02のVer 2.5以降**、`inputs/`フォルダにサブフォルダを2個以上作ってそこへ音声ファイルをスタイルに応じて振り分けて置くと、学習の際にサブディレクトリに応じたスタイルが自動的に作成されます。デフォルトスタイルのみでよい場合や手動でスタイルを後で作成する場合は`inputs/`直下へ入れれば大丈夫です。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_fXCTPuiAWvS",
        "outputId": "4db1fce9-08ba-414a-9d25-0e3cc16479ce"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "08-31 07:54:33 |  INFO  | slice.py:167 | Found 1 audio files.\n",
            "08-31 07:54:33 |WARNING | slice.py:169 | Output directory /content/drive/MyDrive/Style-Bert-VITS2/Data/your_model_name/raw already exists, deleting...\n",
            "Using cache found in /root/.cache/torch/hub/litagin02_silero-vad_master\n",
            "  0%|          | 0/1 [00:00<?, ?it/s]Using cache found in /root/.cache/torch/hub/litagin02_silero-vad_master\n",
            "/root/.cache/torch/hub/litagin02_silero-vad_master/utils_vad.py:127: UserWarning: torchaudio._backend.list_audio_backends has been deprecated. This deprecation is part of a large refactoring effort to transition TorchAudio into a maintenance phase. The decoding and encoding capabilities of PyTorch for both audio and video are being consolidated into TorchCodec. Please see https://github.com/pytorch/audio/issues/3902 for more information. It will be removed from the 2.9 release. \n",
            "  audio_backends = torchaudio.list_audio_backends()\n",
            "/usr/local/lib/python3.12/dist-packages/torchaudio/_backend/utils.py:213: UserWarning: In 2.9, this function's implementation will be changed to use torchaudio.load_with_torchcodec` under the hood. Some parameters like ``normalize``, ``format``, ``buffer_size``, and ``backend`` will be ignored. We recommend that you port your code to rely directly on TorchCodec's decoder instead: https://docs.pytorch.org/torchcodec/stable/generated/torchcodec.decoders.AudioDecoder.html#torchcodec.decoders.AudioDecoder.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torchaudio/_backend/ffmpeg.py:88: UserWarning: torio.io._streaming_media_decoder.StreamingMediaDecoder has been deprecated. This deprecation is part of a large refactoring effort to transition TorchAudio into a maintenance phase. The decoding and encoding capabilities of PyTorch for both audio and video are being consolidated into TorchCodec. Please see https://github.com/pytorch/audio/issues/3902 for more information. It will be removed from the 2.9 release. \n",
            "  s = torchaudio.io.StreamReader(src, format, None, buffer_size)\n",
            "100%|##########| 1/1 [00:00<00:00,  6.02it/s]\n",
            "08-31 07:54:34 |  INFO  | slice.py:265 | Slice done! Total time: 0.09 min, 1 files.\n",
            "08-31 07:54:36 |  INFO  | transcribe.py:157 | Found 1 WAV files\n",
            "08-31 07:54:36 |WARNING | transcribe.py:163 | /content/drive/MyDrive/Style-Bert-VITS2/Data/your_model_name/esd.list exists, backing up to /content/drive/MyDrive/Style-Bert-VITS2/Data/your_model_name/esd.list.bak\n",
            "08-31 07:54:36 |  INFO  | transcribe.py:204 | Loading HF Whisper model ()\n",
            "  0%|          | 0/1 [00:00<?, ?it/s]2025-08-31 07:54:41.154662: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "E0000 00:00:1756626881.174217   20917 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "E0000 00:00:1756626881.180165   20917 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "W0000 00:00:1756626881.195713   20917 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1756626881.195738   20917 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1756626881.195742   20917 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1756626881.195747   20917 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "2025-08-31 07:54:41.200364: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/transformers/utils/hub.py\", line 479, in cached_files\n",
            "    hf_hub_download(\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/huggingface_hub/utils/_validators.py\", line 106, in _inner_fn\n",
            "    validate_repo_id(arg_value)\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/huggingface_hub/utils/_validators.py\", line 160, in validate_repo_id\n",
            "    raise HFValidationError(\n",
            "huggingface_hub.errors.HFValidationError: Repo id must use alphanumeric chars or '-', '_', '.', '--' and '..' are forbidden, '-' and '.' cannot start or end the name, max length is 96: ''.\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/transformers/feature_extraction_utils.py\", line 508, in get_feature_extractor_dict\n",
            "    resolved_feature_extractor_file = cached_file(\n",
            "                                      ^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/transformers/utils/hub.py\", line 321, in cached_file\n",
            "    file = cached_files(path_or_repo_id=path_or_repo_id, filenames=[filename], **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/transformers/utils/hub.py\", line 532, in cached_files\n",
            "    _get_cache_file_to_return(path_or_repo_id, filename, cache_dir, revision, repo_type)\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/transformers/utils/hub.py\", line 144, in _get_cache_file_to_return\n",
            "    resolved_file = try_to_load_from_cache(\n",
            "                    ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/huggingface_hub/utils/_validators.py\", line 106, in _inner_fn\n",
            "    validate_repo_id(arg_value)\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/huggingface_hub/utils/_validators.py\", line 160, in validate_repo_id\n",
            "    raise HFValidationError(\n",
            "huggingface_hub.errors.HFValidationError: Repo id must use alphanumeric chars or '-', '_', '.', '--' and '..' are forbidden, '-' and '.' cannot start or end the name, max length is 96: ''.\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/content/Style-Bert-VITS2/transcribe.py\", line 206, in <module>\n",
            "    results = transcribe_files_with_hf_whisper(\n",
            "              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/content/Style-Bert-VITS2/transcribe.py\", line 63, in transcribe_files_with_hf_whisper\n",
            "    processor: WhisperProcessor = WhisperProcessor.from_pretrained(model_id)\n",
            "                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/transformers/processing_utils.py\", line 1312, in from_pretrained\n",
            "    args = cls._get_arguments_from_pretrained(pretrained_model_name_or_path, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/transformers/processing_utils.py\", line 1371, in _get_arguments_from_pretrained\n",
            "    args.append(attribute_class.from_pretrained(pretrained_model_name_or_path, **kwargs))\n",
            "                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/transformers/feature_extraction_utils.py\", line 382, in from_pretrained\n",
            "    feature_extractor_dict, kwargs = cls.get_feature_extractor_dict(pretrained_model_name_or_path, **kwargs)\n",
            "                                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/transformers/feature_extraction_utils.py\", line 527, in get_feature_extractor_dict\n",
            "    raise OSError(\n",
            "OSError: Can't load feature extractor for ''. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '' is the correct path to a directory containing a preprocessor_config.json file\n",
            "  0%|          | 0/1 [00:09<?, ?it/s]\n"
          ]
        }
      ],
      "source": [
        "# 元となる音声ファイル（wav形式）を入れるディレクトリ\n",
        "input_dir = \"/content/drive/MyDrive/Style-Bert-VITS2/inputs\"\n",
        "# モデル名（話者名）を入力\n",
        "model_name = \"your_model_name\"\n",
        "\n",
        "# こういうふうに書き起こして欲しいという例文（句読点の入れ方・笑い方や固有名詞等）\n",
        "initial_prompt = \"こんにちは。元気、ですかー？ふふっ、私は……ちゃんと元気だよ！\"\n",
        "\n",
        "!python slice.py -i {input_dir} --model_name {model_name}\n",
        "!python transcribe.py --model_name {model_name} --initial_prompt {initial_prompt} --use_hf_whisper"
      ]
    },
    {
      "source": [
        "!python transcribe.py --model_name {model_name} --initial_prompt {initial_prompt} --use_hf_whisper --hf_repo_id \"openai/whisper-large-v3\""
      ],
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1CBe2aDLHTFc",
        "outputId": "d2fddf42-8d63-4c09-e26c-2f7e69b0f277"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "08-31 07:52:44 |  INFO  | transcribe.py:157 | Found 1 WAV files\n",
            "08-31 07:52:44 |  INFO  | transcribe.py:204 | Loading HF Whisper model (openai/whisper-large-v3)\n",
            "\r  0%|          | 0/1 [00:00<?, ?it/s]2025-08-31 07:52:47.861515: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "E0000 00:00:1756626767.882547   20400 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "E0000 00:00:1756626767.889860   20400 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "W0000 00:00:1756626767.906254   20400 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1756626767.906282   20400 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1756626767.906287   20400 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1756626767.906292   20400 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "2025-08-31 07:52:47.911975: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "\n",
            "preprocessor_config.json: 100% 340/340 [00:00<00:00, 2.08MB/s]\n",
            "\n",
            "tokenizer_config.json: 283kB [00:00, 312MB/s]\n",
            "\n",
            "vocab.json: 1.04MB [00:00, 29.9MB/s]\n",
            "\n",
            "tokenizer.json: 2.48MB [00:00, 159MB/s]\n",
            "\n",
            "merges.txt: 494kB [00:00, 133MB/s]\n",
            "\n",
            "normalizer.json: 52.7kB [00:00, 122MB/s]\n",
            "\n",
            "added_tokens.json: 34.6kB [00:00, 108MB/s]\n",
            "\n",
            "special_tokens_map.json: 2.07kB [00:00, 14.9MB/s]\n",
            "08-31 07:52:54 |  INFO  | transcribe.py:70 | generate_kwargs: {'language': 'ja', 'do_sample': False, 'num_beams': 1, 'no_repeat_ngram_size': 10}, loading pipeline...\n",
            "\n",
            "config.json: 1.27kB [00:00, 7.95MB/s]\n",
            "\n",
            "model.safetensors:   0% 0.00/3.09G [00:00<?, ?B/s]\u001b[A\n",
            "model.safetensors:   0% 153k/3.09G [00:01<7:15:08, 118kB/s]\u001b[A\n",
            "model.safetensors:   0% 1.74M/3.09G [00:01<30:44, 1.67MB/s]\u001b[A\n",
            "model.safetensors:   0% 5.01M/3.09G [00:01<10:36, 4.84MB/s]\u001b[A\n",
            "model.safetensors:   1% 23.9M/3.09G [00:01<01:45, 28.9MB/s]\u001b[A\n",
            "model.safetensors:   2% 58.7M/3.09G [00:01<00:39, 76.3MB/s]\u001b[A\n",
            "model.safetensors:   3% 87.4M/3.09G [00:01<00:27, 108MB/s] \u001b[A\n",
            "model.safetensors:   3% 106M/3.09G [00:02<00:24, 123MB/s] \u001b[A\n",
            "model.safetensors:   4% 129M/3.09G [00:02<00:24, 120MB/s]\u001b[A\n",
            "model.safetensors:   5% 147M/3.09G [00:02<00:27, 106MB/s]\u001b[A\n",
            "model.safetensors:   5% 167M/3.09G [00:02<00:23, 122MB/s]\u001b[A\n",
            "model.safetensors:   6% 183M/3.09G [00:02<00:25, 115MB/s]\u001b[A\n",
            "model.safetensors:   7% 201M/3.09G [00:02<00:24, 117MB/s]\u001b[A\n",
            "model.safetensors:   7% 221M/3.09G [00:03<00:25, 114MB/s]\u001b[A\n",
            "model.safetensors:   8% 242M/3.09G [00:03<00:25, 111MB/s]\u001b[A\n",
            "model.safetensors:   9% 264M/3.09G [00:03<00:21, 131MB/s]\u001b[A\n",
            "model.safetensors:   9% 281M/3.09G [00:03<00:33, 84.7MB/s]\u001b[A\n",
            "model.safetensors:  10% 296M/3.09G [00:03<00:29, 95.2MB/s]\u001b[A\n",
            "model.safetensors:  10% 316M/3.09G [00:04<00:28, 96.2MB/s]\u001b[A\n",
            "model.safetensors:  11% 338M/3.09G [00:04<00:30, 89.5MB/s]\u001b[A\n",
            "model.safetensors:  12% 370M/3.09G [00:04<00:21, 126MB/s] \u001b[A\n",
            "model.safetensors:  13% 390M/3.09G [00:08<02:28, 18.2MB/s]\u001b[A\n",
            "model.safetensors:  15% 460M/3.09G [00:08<01:04, 40.9MB/s]\u001b[A\n",
            "model.safetensors:  16% 497M/3.09G [00:09<01:00, 43.0MB/s]\u001b[A\n",
            "model.safetensors:  17% 519M/3.09G [00:09<01:00, 42.4MB/s]\u001b[A\n",
            "model.safetensors:  17% 532M/3.09G [00:14<03:06, 13.7MB/s]\u001b[A\n",
            "model.safetensors:  19% 573M/3.09G [00:14<01:52, 22.3MB/s]\u001b[A\n",
            "model.safetensors:  19% 593M/3.09G [00:23<05:23, 7.72MB/s]\u001b[A\n",
            "model.safetensors:  21% 644M/3.09G [00:23<02:58, 13.7MB/s]\u001b[A\n",
            "model.safetensors:  25% 774M/3.09G [00:23<01:06, 34.7MB/s]\u001b[A\n",
            "model.safetensors:  28% 871M/3.09G [00:29<01:36, 23.0MB/s]\u001b[A\n",
            "model.safetensors:  30% 937M/3.09G [00:29<01:08, 31.5MB/s]\u001b[A\n",
            "model.safetensors:  32% 983M/3.09G [00:30<00:57, 36.6MB/s]\u001b[A\n",
            "model.safetensors:  33% 1.02G/3.09G [00:30<00:50, 40.6MB/s]\u001b[A\n",
            "model.safetensors:  34% 1.05G/3.09G [00:36<01:47, 18.9MB/s]\u001b[A\n",
            "model.safetensors:  35% 1.07G/3.09G [00:36<01:38, 20.4MB/s]\u001b[A\n",
            "model.safetensors:  35% 1.09G/3.09G [00:36<01:26, 23.1MB/s]\u001b[A\n",
            "model.safetensors:  37% 1.13G/3.09G [00:36<00:56, 34.9MB/s]\u001b[A\n",
            "model.safetensors:  38% 1.16G/3.09G [00:37<00:42, 44.9MB/s]\u001b[A\n",
            "model.safetensors:  39% 1.19G/3.09G [00:37<00:33, 57.4MB/s]\u001b[A\n",
            "model.safetensors:  39% 1.21G/3.09G [00:37<00:29, 64.0MB/s]\u001b[A\n",
            "model.safetensors:  41% 1.25G/3.09G [00:37<00:20, 88.5MB/s]\u001b[A\n",
            "model.safetensors:  41% 1.28G/3.09G [00:38<00:23, 78.0MB/s]\u001b[A\n",
            "model.safetensors:  42% 1.29G/3.09G [00:38<00:20, 86.1MB/s]\u001b[A\n",
            "model.safetensors:  43% 1.32G/3.09G [00:38<00:17, 104MB/s] \u001b[A\n",
            "model.safetensors:  43% 1.34G/3.09G [00:38<00:18, 93.5MB/s]\u001b[A\n",
            "model.safetensors:  44% 1.37G/3.09G [00:38<00:16, 106MB/s] \u001b[A\n",
            "model.safetensors:  45% 1.38G/3.09G [00:38<00:16, 104MB/s]\u001b[A\n",
            "model.safetensors:  45% 1.40G/3.09G [00:39<00:15, 107MB/s]\u001b[A\n",
            "model.safetensors:  46% 1.42G/3.09G [00:39<00:12, 128MB/s]\u001b[A\n",
            "model.safetensors:  47% 1.44G/3.09G [00:39<00:12, 128MB/s]\u001b[A\n",
            "model.safetensors:  47% 1.46G/3.09G [00:40<00:34, 47.3MB/s]\u001b[A\n",
            "model.safetensors:  48% 1.48G/3.09G [00:40<00:31, 51.3MB/s]\u001b[A\n",
            "model.safetensors:  48% 1.49G/3.09G [00:40<00:26, 59.2MB/s]\u001b[A\n",
            "model.safetensors:  49% 1.51G/3.09G [00:40<00:19, 80.8MB/s]\u001b[A\n",
            "model.safetensors:  51% 1.56G/3.09G [00:40<00:11, 135MB/s] \u001b[A\n",
            "model.safetensors:  51% 1.59G/3.09G [00:41<00:09, 153MB/s]\u001b[A\n",
            "model.safetensors:  53% 1.62G/3.09G [00:41<00:09, 158MB/s]\u001b[A\n",
            "model.safetensors:  53% 1.65G/3.09G [00:41<00:08, 175MB/s]\u001b[A\n",
            "model.safetensors:  54% 1.68G/3.09G [00:41<00:07, 200MB/s]\u001b[A\n",
            "model.safetensors:  55% 1.71G/3.09G [00:42<00:13, 100MB/s]\u001b[A\n",
            "model.safetensors:  56% 1.73G/3.09G [00:42<00:12, 113MB/s]\u001b[A\n",
            "model.safetensors:  57% 1.77G/3.09G [00:42<00:08, 155MB/s]\u001b[A\n",
            "model.safetensors:  58% 1.80G/3.09G [00:42<00:07, 177MB/s]\u001b[A\n",
            "model.safetensors:  60% 1.84G/3.09G [00:42<00:10, 115MB/s]\u001b[A\n",
            "model.safetensors:  60% 1.87G/3.09G [00:43<00:09, 134MB/s]\u001b[A\n",
            "model.safetensors:  62% 1.91G/3.09G [00:43<00:09, 124MB/s]\u001b[A\n",
            "model.safetensors:  63% 1.93G/3.09G [00:43<00:09, 119MB/s]\u001b[A\n",
            "model.safetensors:  63% 1.96G/3.09G [00:43<00:08, 133MB/s]\u001b[A\n",
            "model.safetensors:  65% 2.00G/3.09G [00:43<00:06, 160MB/s]\u001b[A\n",
            "model.safetensors:  66% 2.03G/3.09G [00:44<00:05, 186MB/s]\u001b[A\n",
            "model.safetensors:  67% 2.05G/3.09G [00:44<00:09, 106MB/s]\u001b[A\n",
            "model.safetensors:  67% 2.07G/3.09G [00:44<00:09, 109MB/s]\u001b[A\n",
            "model.safetensors:  68% 2.11G/3.09G [00:44<00:06, 143MB/s]\u001b[A\n",
            "model.safetensors:  69% 2.14G/3.09G [00:47<00:25, 37.5MB/s]\u001b[A\n",
            "model.safetensors:  70% 2.17G/3.09G [00:47<00:17, 51.7MB/s]\u001b[A\n",
            "model.safetensors:  71% 2.20G/3.09G [00:47<00:14, 59.8MB/s]\u001b[A\n",
            "model.safetensors:  73% 2.26G/3.09G [00:47<00:08, 99.8MB/s]\u001b[A\n",
            "model.safetensors:  74% 2.29G/3.09G [00:47<00:07, 107MB/s] \u001b[A\n",
            "model.safetensors:  75% 2.32G/3.09G [00:48<00:07, 104MB/s]\u001b[A\n",
            "model.safetensors:  76% 2.34G/3.09G [00:48<00:06, 108MB/s]\u001b[A\n",
            "model.safetensors:  77% 2.37G/3.09G [00:48<00:05, 133MB/s]\u001b[A\n",
            "model.safetensors:  78% 2.40G/3.09G [00:48<00:06, 99.8MB/s]\u001b[A\n",
            "model.safetensors:  79% 2.43G/3.09G [00:48<00:05, 122MB/s] \u001b[A\n",
            "model.safetensors:  79% 2.45G/3.09G [00:51<00:19, 33.4MB/s]\u001b[A\n",
            "model.safetensors:  81% 2.50G/3.09G [00:51<00:11, 53.7MB/s]\u001b[A\n",
            "model.safetensors:  82% 2.53G/3.09G [00:51<00:07, 69.9MB/s]\u001b[A\n",
            "model.safetensors:  83% 2.56G/3.09G [00:51<00:05, 88.1MB/s]\u001b[A\n",
            "model.safetensors:  84% 2.59G/3.09G [00:51<00:05, 95.9MB/s]\u001b[A\n",
            "model.safetensors:  85% 2.61G/3.09G [00:51<00:04, 97.1MB/s]\u001b[A\n",
            "model.safetensors:  85% 2.64G/3.09G [00:52<00:03, 113MB/s] \u001b[A\n",
            "model.safetensors:  86% 2.66G/3.09G [00:52<00:04, 97.9MB/s]\u001b[A\n",
            "model.safetensors:  87% 2.69G/3.09G [00:52<00:03, 127MB/s] \u001b[A\n",
            "model.safetensors:  88% 2.72G/3.09G [00:52<00:03, 116MB/s]\u001b[A\n",
            "model.safetensors:  89% 2.74G/3.09G [00:53<00:03, 104MB/s]\u001b[A\n",
            "model.safetensors:  89% 2.76G/3.09G [00:53<00:06, 53.8MB/s]\u001b[A\n",
            "model.safetensors:  90% 2.77G/3.09G [00:55<00:10, 30.6MB/s]\u001b[A\n",
            "model.safetensors:  90% 2.79G/3.09G [00:55<00:07, 40.5MB/s]\u001b[A\n",
            "model.safetensors:  92% 2.85G/3.09G [00:55<00:03, 76.7MB/s]\u001b[A\n",
            "model.safetensors:  93% 2.87G/3.09G [00:55<00:03, 70.8MB/s]\u001b[A\n",
            "model.safetensors:  94% 2.89G/3.09G [00:56<00:02, 80.9MB/s]\u001b[A\n",
            "model.safetensors:  94% 2.92G/3.09G [00:56<00:01, 91.7MB/s]\u001b[A\n",
            "model.safetensors:  95% 2.94G/3.09G [00:56<00:01, 107MB/s] \u001b[A\n",
            "model.safetensors:  96% 2.97G/3.09G [00:56<00:01, 113MB/s]\u001b[A\n",
            "model.safetensors:  98% 3.01G/3.09G [00:56<00:00, 148MB/s]\u001b[A\n",
            "model.safetensors:  99% 3.05G/3.09G [00:56<00:00, 175MB/s]\u001b[A\n",
            "model.safetensors: 100% 3.09G/3.09G [00:57<00:00, 53.9MB/s]\n",
            "\n",
            "generation_config.json: 3.90kB [00:00, 22.5MB/s]\n",
            "Device set to use cuda\n",
            "Using `chunk_length_s` is very experimental with seq2seq models. The results will not necessarily be entirely accurate and will have caveats. More information: https://github.com/huggingface/transformers/pull/20104. Ignore this warning with pipeline(..., ignore_warning=True). To use Whisper for long-form transcription, use rather the model's `generate` method directly as the model relies on it's own chunking mechanism (cf. Whisper original paper, section 3.8. Long-form Transcription).\n",
            "08-31 07:54:08 |  INFO  | transcribe.py:81 | Loaded pipeline\n",
            "100%|##########| 1/1 [01:27<00:00, 87.43s/it]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "CixcFueo_sJw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j7vEWewoAWvS"
      },
      "source": [
        "成功したらそのまま3へ進んでください"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z3AC-3zpAWvS"
      },
      "source": [
        "### 2.2 音声ファイルと書き起こしデータがすでにある場合\n",
        "\n",
        "指示に従って適切にデータセットを配置してください。\n",
        "\n",
        "次のセルを実行して、学習データをいれるフォルダ（1で設定した`dataset_root`）を作成します。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "esCNJl704h52"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "\n",
        "os.makedirs(dataset_root, exist_ok=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aaDgJCjCAWvT"
      },
      "source": [
        "まず音声データと、書き起こしテキストを用意してください。\n",
        "\n",
        "それを次のように配置します。\n",
        "```\n",
        "├── Data/\n",
        "│   ├── {モデルの名前}\n",
        "│   │   ├── esd.list\n",
        "│   │   ├── raw/\n",
        "│   │   │   ├── foo.wav\n",
        "│   │   │   ├── bar.mp3\n",
        "│   │   │   ├── style1/\n",
        "│   │   │   │   ├── baz.wav\n",
        "│   │   │   │   ├── qux.wav\n",
        "│   │   │   ├── style2/\n",
        "│   │   │   │   ├── corge.wav\n",
        "│   │   │   │   ├── grault.wav\n",
        "...\n",
        "```\n",
        "\n",
        "### 配置の仕方\n",
        "- 上のように配置すると、`style1/`と`style2/`フォルダの内部（直下以外も含む）に入っている音声ファイルたちから、自動的にデフォルトスタイルに加えて`style1`と`style2`というスタイルが作成されます\n",
        "- 特にスタイルを作る必要がない場合や、スタイル分類機能等でスタイルを作る場合は、`raw/`フォルダ直下に全てを配置してください。このように`raw/`のサブディレクトリの個数が0または1の場合は、スタイルはデフォルトスタイルのみが作成されます。\n",
        "- 音声ファイルのフォーマットはwav形式以外にもmp3等の多くの音声ファイルに対応しています\n",
        "\n",
        "### 書き起こしファイル`esd.list`\n",
        "\n",
        "`Data/{モデルの名前}/esd.list` ファイルには、以下のフォーマットで各音声ファイルの情報を記述してください。\n",
        "\n",
        "\n",
        "```\n",
        "path/to/audio.wav(wavファイル以外でもこう書く)|{話者名}|{言語ID、ZHかJPかEN}|{書き起こしテキスト}\n",
        "```\n",
        "\n",
        "- ここで、最初の`path/to/audio.wav`は、`raw/`からの相対パスです。つまり、`raw/foo.wav`の場合は`foo.wav`、`raw/style1/bar.wav`の場合は`style1/bar.wav`となります。\n",
        "- 拡張子がwavでない場合でも、`esd.list`には`wav`と書いてください、つまり、`raw/bar.mp3`の場合でも`bar.wav`と書いてください。\n",
        "\n",
        "\n",
        "例：\n",
        "```\n",
        "foo.wav|hanako|JP|こんにちは、元気ですか？\n",
        "bar.wav|taro|JP|はい、聞こえています……。何か用ですか？\n",
        "style1/baz.wav|hanako|JP|今日はいい天気ですね。\n",
        "style1/qux.wav|taro|JP|はい、そうですね。\n",
        "...\n",
        "english_teacher.wav|Mary|EN|How are you? I'm fine, thank you, and you?\n",
        "...\n",
        "```\n",
        "もちろん日本語話者の単一話者データセットでも構いません。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5r85-W20ECcr"
      },
      "source": [
        "## 3. 学習の前処理\n",
        "\n",
        "次に学習の前処理を行います。必要なパラメータをここで指定します。次のセルに設定等を入力して実行してください。「～～かどうか」は`True`もしくは`False`を指定してください。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "id": "CXR7kjuF5GlE"
      },
      "outputs": [],
      "source": [
        "# 上でつけたフォルダの名前`Data/{model_name}/`\n",
        "model_name = \"your_model_name\"\n",
        "\n",
        "# JP-Extra （日本語特化版）を使うかどうか。日本語の能力が向上する代わりに英語と中国語は使えなくなります。\n",
        "use_jp_extra = True\n",
        "\n",
        "# 学習のバッチサイズ。VRAMのはみ出具合に応じて調整してください。\n",
        "batch_size = 4\n",
        "\n",
        "# 学習のエポック数（データセットを合計何周するか）。\n",
        "# 100で多すぎるほどかもしれませんが、もっと多くやると質が上がるのかもしれません。\n",
        "epochs = 100\n",
        "\n",
        "# 保存頻度。何ステップごとにモデルを保存するか。分からなければデフォルトのままで。\n",
        "save_every_steps = 1000\n",
        "\n",
        "# 音声ファイルの音量を正規化するかどうか\n",
        "normalize = False\n",
        "\n",
        "# 音声ファイルの開始・終了にある無音区間を削除するかどうか\n",
        "trim = False\n",
        "\n",
        "# 読みのエラーが出た場合にどうするか。\n",
        "# \"raise\"ならテキスト前処理が終わったら中断、\"skip\"なら読めない行は学習に使わない、\"use\"なら無理やり使う\n",
        "yomi_error = \"skip\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BFZdLTtpAWvT"
      },
      "source": [
        "上のセルが実行されたら、次のセルを実行して学習の前処理を行います。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xMVaOIPLabV5",
        "outputId": "4b065809-69d8-4486-91ce-b9da2d83a22e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "08-31 08:15:03 |  INFO  | train.py:72 | Step 1: start initialization...\n",
            "model_name: your_model_name, batch_size: 4, epochs: 100, save_every_steps: 1000, freeze_ZH_bert: False, freeze_JP_bert: False, freeze_EN_bert: False, freeze_style: False, freeze_decoder: False, use_jp_extra: True\n",
            "08-31 08:15:03 |WARNING | train.py:103 | Step 1: /content/drive/MyDrive/Style-Bert-VITS2/Data/your_model_name/models already exists, so copy it to backup to /content/drive/MyDrive/Style-Bert-VITS2/Data/your_model_name/models_backup\n",
            "08-31 08:15:22 |SUCCESS | train.py:132 | Step 1: initialization finished.\n",
            "08-31 08:15:22 |  INFO  | train.py:137 | Step 2: start resampling...\n",
            "08-31 08:15:22 |  INFO  | subprocess.py:23 | Running: resample.py -i /content/drive/MyDrive/Style-Bert-VITS2/Data/your_model_name/raw -o /content/drive/MyDrive/Style-Bert-VITS2/Data/your_model_name/wavs --num_processes 2 --sr 44100\n",
            "08-31 08:15:27 |SUCCESS | subprocess.py:38 | Success: resample.py -i /content/drive/MyDrive/Style-Bert-VITS2/Data/your_model_name/raw -o /content/drive/MyDrive/Style-Bert-VITS2/Data/your_model_name/wavs --num_processes 2 --sr 44100\n",
            "08-31 08:15:27 |SUCCESS | train.py:163 | Step 2: resampling finished.\n",
            "08-31 08:15:27 |  INFO  | train.py:170 | Step 3: start preprocessing text...\n",
            "08-31 08:15:27 |  INFO  | subprocess.py:23 | Running: preprocess_text.py --config-path /content/drive/MyDrive/Style-Bert-VITS2/Data/your_model_name/config.json --transcription-path /content/drive/MyDrive/Style-Bert-VITS2/Data/your_model_name/esd.list --train-path /content/drive/MyDrive/Style-Bert-VITS2/Data/your_model_name/train.list --val-path /content/drive/MyDrive/Style-Bert-VITS2/Data/your_model_name/val.list --val-per-lang 0 --yomi_error skip --correct_path --use_jp_extra\n",
            "08-31 08:15:39 |WARNING | subprocess.py:36 | Warning: preprocess_text.py --config-path /content/drive/MyDrive/Style-Bert-VITS2/Data/your_model_name/config.json --transcription-path /content/drive/MyDrive/Style-Bert-VITS2/Data/your_model_name/esd.list --train-path /content/drive/MyDrive/Style-Bert-VITS2/Data/your_model_name/train.list --val-path /content/drive/MyDrive/Style-Bert-VITS2/Data/your_model_name/val.list --val-per-lang 0 --yomi_error skip --correct_path --use_jp_extra\n",
            "2025-08-31 08:15:34.165264: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "E0000 00:00:1756628134.185662   26340 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "E0000 00:00:1756628134.191862   26340 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "W0000 00:00:1756628134.207545   26340 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1756628134.207571   26340 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1756628134.207574   26340 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1756628134.207577   26340 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "\n",
            "08-31 08:15:39 |WARNING | train.py:205 | Step 3: preprocessing text finished with stderr.\n",
            "08-31 08:15:39 |  INFO  | train.py:215 | Step 4: start bert_gen...\n",
            "08-31 08:15:39 |  INFO  | subprocess.py:23 | Running: bert_gen.py --config /content/drive/MyDrive/Style-Bert-VITS2/Data/your_model_name/config.json\n",
            "08-31 08:15:42 |SUCCESS | subprocess.py:38 | Success: bert_gen.py --config /content/drive/MyDrive/Style-Bert-VITS2/Data/your_model_name/config.json\n",
            "08-31 08:15:42 |SUCCESS | train.py:229 | Step 4: bert_gen finished.\n",
            "08-31 08:15:42 |  INFO  | train.py:234 | Step 5: start style_gen...\n",
            "08-31 08:15:42 |  INFO  | subprocess.py:23 | Running: style_gen.py --config /content/drive/MyDrive/Style-Bert-VITS2/Data/your_model_name/config.json --num_processes 2\n",
            "08-31 08:15:53 |WARNING | subprocess.py:36 | Warning: style_gen.py --config /content/drive/MyDrive/Style-Bert-VITS2/Data/your_model_name/config.json --num_processes 2\n",
            "/usr/local/lib/python3.12/dist-packages/pyannote/audio/core/io.py:212: UserWarning: torchaudio._backend.list_audio_backends has been deprecated. This deprecation is part of a large refactoring effort to transition TorchAudio into a maintenance phase. The decoding and encoding capabilities of PyTorch for both audio and video are being consolidated into TorchCodec. Please see https://github.com/pytorch/audio/issues/3902 for more information. It will be removed from the 2.9 release. \n",
            "  torchaudio.list_audio_backends()\n",
            "/usr/local/lib/python3.12/dist-packages/pyannote/audio/utils/reproducibility.py:74: ReproducibilityWarning: TensorFloat-32 (TF32) has been disabled as it might lead to reproducibility issues and lower accuracy.\n",
            "It can be re-enabled by calling\n",
            "   >>> import torch\n",
            "   >>> torch.backends.cuda.matmul.allow_tf32 = True\n",
            "   >>> torch.backends.cudnn.allow_tf32 = True\n",
            "See https://github.com/pyannote/pyannote-audio/issues/1370 for more details.\n",
            "\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torchaudio/_backend/utils.py:213: UserWarning: In 2.9, this function's implementation will be changed to use torchaudio.load_with_torchcodec` under the hood. Some parameters like ``normalize``, ``format``, ``buffer_size``, and ``backend`` will be ignored. We recommend that you port your code to rely directly on TorchCodec's decoder instead: https://docs.pytorch.org/torchcodec/stable/generated/torchcodec.decoders.AudioDecoder.html#torchcodec.decoders.AudioDecoder.\n",
            "  warnings.warn(\n",
            "\n",
            "08-31 08:15:53 |WARNING | train.py:252 | Step 5: style_gen finished with stderr.\n",
            "08-31 08:15:53 |SUCCESS | train.py:321 | Success: All preprocess finished!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(True, 'Success: 全ての前処理が完了しました。ターミナルを確認しておかしいところがないか確認するのをおすすめします。')"
            ]
          },
          "metadata": {},
          "execution_count": 29
        }
      ],
      "source": [
        "from gradio_tabs.train import preprocess_all\n",
        "from style_bert_vits2.nlp.japanese import pyopenjtalk_worker\n",
        "\n",
        "\n",
        "pyopenjtalk_worker.initialize_worker()\n",
        "\n",
        "preprocess_all(\n",
        "    model_name=model_name,\n",
        "    batch_size=batch_size,\n",
        "    epochs=epochs,\n",
        "    save_every_steps=save_every_steps,\n",
        "    num_processes=2,\n",
        "    normalize=normalize,\n",
        "    trim=trim,\n",
        "    freeze_EN_bert=False,\n",
        "    freeze_JP_bert=False,\n",
        "    freeze_ZH_bert=False,\n",
        "    freeze_style=False,\n",
        "    freeze_decoder=False,\n",
        "    use_jp_extra=use_jp_extra,\n",
        "    val_per_lang=0,\n",
        "    log_interval=200,\n",
        "    yomi_error=yomi_error,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sVhwI5C-AWvT"
      },
      "source": [
        "## 4. 学習\n",
        "\n",
        "前処理が正常に終わったら、学習を行います。次のセルを実行すると学習が始まります。\n",
        "\n",
        "学習の結果は、上で指定した`save_every_steps`の間隔で、Google Driveの中の`Style-Bert-VITS2/Data/{モデルの名前}/model_assets/`フォルダに保存されます。\n",
        "\n",
        "このフォルダをダウンロードし、ローカルのStyle-Bert-VITS2の`model_assets`フォルダに上書きすれば、学習結果を使うことができます。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "laieKrbEb6Ij",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 384
        },
        "outputId": "992fb727-cf8d-4efe-f29e-3d5d37e4796d"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "ModuleNotFoundError",
          "evalue": "No module named 'gradio_tabs'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-1348128769.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0myaml\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mgradio_tabs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mget_path\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0mpaths\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_path\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'gradio_tabs'",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ],
          "errorDetails": {
            "actions": [
              {
                "action": "open_url",
                "actionText": "Open Examples",
                "url": "/notebooks/snippets/importing_libraries.ipynb"
              }
            ]
          }
        }
      ],
      "source": [
        "# 上でつけたモデル名を入力。学習を途中からする場合はきちんとモデルが保存されているフォルダ名を入力。\n",
        "model_name = \"your_model_name\"\n",
        "\n",
        "\n",
        "import yaml\n",
        "from gradio_tabs.train import get_path\n",
        "\n",
        "paths = get_path(model_name)\n",
        "dataset_path = str(paths.dataset_path)\n",
        "config_path = str(paths.config_path)\n",
        "\n",
        "with open(\"default_config.yml\", \"r\", encoding=\"utf-8\") as f:\n",
        "    yml_data = yaml.safe_load(f)\n",
        "yml_data[\"model_name\"] = model_name\n",
        "with open(\"config.yml\", \"w\", encoding=\"utf-8\") as f:\n",
        "    yaml.dump(yml_data, f, allow_unicode=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JqGeHNabAWvT",
        "outputId": "6d98dfef-062c-434f-fd1b-671ebb1d5c9b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2025-08-31 08:17:36.391591: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "E0000 00:00:1756628256.412058   26909 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "E0000 00:00:1756628256.418136   26909 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "W0000 00:00:1756628256.433678   26909 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1756628256.433703   26909 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1756628256.433708   26909 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1756628256.433713   26909 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "08-31 08:17:44 |  INFO  | train_ms_jp_extra.py:118 | Loading configuration from config 0\n",
            "08-31 08:17:44 |  INFO  | train_ms_jp_extra.py:118 | Loading configuration from config localhost\n",
            "08-31 08:17:44 |  INFO  | train_ms_jp_extra.py:118 | Loading configuration from config 10086\n",
            "08-31 08:17:44 |  INFO  | train_ms_jp_extra.py:118 | Loading configuration from config 0\n",
            "08-31 08:17:44 |  INFO  | train_ms_jp_extra.py:118 | Loading configuration from config 1\n",
            "08-31 08:17:44 |  INFO  | train_ms_jp_extra.py:120 | Loading environment variables \n",
            "MASTER_ADDR: localhost,\n",
            "MASTER_PORT: 10086,\n",
            "WORLD_SIZE: 1,\n",
            "RANK: 0,\n",
            "LOCAL_RANK: 0\n",
            "08-31 08:17:44 |  INFO  | default_style.py:54 | At least 2 subdirectories are required for generating style vectors with respect to them, found 0.\n",
            "08-31 08:17:44 |  INFO  | default_style.py:57 | Generating only neutral style vector instead.\n",
            "08-31 08:17:44 |  INFO  | default_style.py:28 | Saved mean style vector to /content/drive/MyDrive/Style-Bert-VITS2/model_assets/your_model_name\n",
            "08-31 08:17:44 |  INFO  | default_style.py:36 | Saved style config to /content/drive/MyDrive/Style-Bert-VITS2/model_assets/your_model_name/config.json\n",
            "08-31 08:17:44 |WARNING | __init__.py:247 | /content/Style-Bert-VITS2/style_bert_vits2/models/utils is not a git repository, therefore hash value comparison will be ignored.\n",
            "08-31 08:17:44 |  INFO  | data_utils.py:69 | Init dataset...\n",
            "100% 1/1 [00:00<00:00, 6864.65it/s]\n",
            "08-31 08:17:44 |  INFO  | data_utils.py:84 | skipped: 0, total: 1\n",
            "08-31 08:17:44 |  INFO  | data_utils.py:374 | Bucket warning \n",
            "08-31 08:17:44 |  INFO  | data_utils.py:348 | Bucket info: [4]\n",
            "08-31 08:17:44 |  INFO  | data_utils.py:69 | Init dataset...\n",
            "0it [00:00, ?it/s]\n",
            "08-31 08:17:44 |  INFO  | data_utils.py:84 | skipped: 0, total: 0\n",
            "08-31 08:17:44 |  INFO  | train_ms_jp_extra.py:287 | Using noise scaled MAS for VITS2\n",
            "08-31 08:17:46 |WARNING | safetensors.py:43 | Missing key: enc_p.style_proj.weight\n",
            "08-31 08:17:46 |WARNING | safetensors.py:43 | Missing key: enc_p.style_proj.bias\n",
            "08-31 08:17:46 |WARNING | safetensors.py:43 | Missing key: emb_g.weight\n",
            "08-31 08:17:46 |  INFO  | safetensors.py:49 | Loaded '/content/drive/MyDrive/Style-Bert-VITS2/Data/your_model_name/models/G_0.safetensors'\n",
            "08-31 08:17:46 |  INFO  | safetensors.py:49 | Loaded '/content/drive/MyDrive/Style-Bert-VITS2/Data/your_model_name/models/D_0.safetensors'\n",
            "08-31 08:17:46 |  INFO  | safetensors.py:49 | Loaded '/content/drive/MyDrive/Style-Bert-VITS2/Data/your_model_name/models/WD_0.safetensors'\n",
            "08-31 08:17:46 |  INFO  | train_ms_jp_extra.py:505 | Loaded the pretrained models.\n",
            "08-31 08:17:49 |  INFO  | train_ms_jp_extra.py:553 | Start training.\n",
            "Epoch 100(0%)/100: 100%|##########| 100/100 [02:51<00:00,  1.68s/it]08-31 08:20:41 |  INFO  | checkpoints.py:111 | Saving model and optimizer state at iteration 100 to /content/drive/MyDrive/Style-Bert-VITS2/Data/your_model_name/models/G_100.pth\n",
            "08-31 08:20:44 |  INFO  | checkpoints.py:111 | Saving model and optimizer state at iteration 100 to /content/drive/MyDrive/Style-Bert-VITS2/Data/your_model_name/models/D_100.pth\n",
            "08-31 08:20:47 |  INFO  | checkpoints.py:111 | Saving model and optimizer state at iteration 100 to /content/drive/MyDrive/Style-Bert-VITS2/Data/your_model_name/models/WD_100.pth\n",
            "08-31 08:20:47 |  INFO  | safetensors.py:90 | Saved safetensors to /content/drive/MyDrive/Style-Bert-VITS2/model_assets/your_model_name/your_model_name_e100_s100.safetensors\n",
            "Epoch 100(0%)/100: 100%|##########| 100/100 [02:58<00:00,  1.79s/it]\n",
            "[rank0]:[W831 08:20:49.093012946 ProcessGroupNCCL.cpp:1538] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())\n"
          ]
        }
      ],
      "source": [
        "# 日本語特化版を「使う」場合\n",
        "!python train_ms_jp_extra.py --config {config_path} --model {dataset_path} --assets_root {assets_root}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rVbjh-WPAWvU"
      },
      "outputs": [],
      "source": [
        "# 日本語特化版を「使わない」場合\n",
        "!python train_ms.py --config {config_path} --model {dataset_path} --assets_root {assets_root}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c7g0hrdeP1Tl",
        "outputId": "b6fb545f-d252-42b9-cc5b-8b85670cfdc8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2025-08-31 07:24:22.259310: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "E0000 00:00:1756625062.279522   13119 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "E0000 00:00:1756625062.285635   13119 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "W0000 00:00:1756625062.301315   13119 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1756625062.301344   13119 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1756625062.301348   13119 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1756625062.301351   13119 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "2025-08-31 07:24:22.306031: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "08-31 07:24:25 | DEBUG  | __init__.py:130 | pyopenjtalk worker server started\n",
            "08-31 07:24:30 |WARNING | tts_model.py:654 | No model files found in /content/drive/MyDrive/Style-Bert-VITS2/model_assets/your_model_name, so skip it\n",
            "08-31 07:24:30 | ERROR  | inference.py:353 | モデルが見つかりませんでした。/content/drive/MyDrive/Style-Bert-VITS2/model_assetsにモデルを置いてください。\n",
            "08-31 07:24:32 |WARNING | tts_model.py:654 | No model files found in /content/drive/MyDrive/Style-Bert-VITS2/model_assets/your_model_name, so skip it\n",
            "08-31 07:24:32 | ERROR  | merge.py:1032 | モデルが見つかりませんでした。/content/drive/MyDrive/Style-Bert-VITS2/model_assetsにモデルを置いてください。\n",
            "08-31 07:24:32 | ERROR  | convert_onnx.py:44 | モデルが見つかりませんでした。/content/drive/MyDrive/Style-Bert-VITS2/model_assetsにモデルを置いてください。\n",
            "* Running on local URL:  http://127.0.0.1:7860\n",
            "* Running on public URL: https://a8adc676136a5ab084.gradio.live\n",
            "\n",
            "This share link expires in 1 week. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n",
            "Keyboard interruption in main thread... closing server.\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/gradio/blocks.py\", line 3158, in block_thread\n",
            "    time.sleep(0.1)\n",
            "KeyboardInterrupt\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/content/Style-Bert-VITS2/app.py\", line 68, in <module>\n",
            "    app.launch(\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/gradio/blocks.py\", line 3055, in launch\n",
            "    self.block_thread()\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/gradio/blocks.py\", line 3162, in block_thread\n",
            "    self.server.close()\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/gradio/http_server.py\", line 69, in close\n",
            "    self.thread.join(timeout=5)\n",
            "  File \"/usr/lib/python3.12/threading.py\", line 1153, in join\n",
            "    self._wait_for_tstate_lock(timeout=max(timeout, 0))\n",
            "  File \"/usr/lib/python3.12/threading.py\", line 1169, in _wait_for_tstate_lock\n",
            "    if lock.acquire(block, timeout):\n",
            "       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "KeyboardInterrupt\n",
            "Killing tunnel 127.0.0.1:7860 <> https://a8adc676136a5ab084.gradio.live\n",
            "08-31 07:27:48 | DEBUG  | __init__.py:147 | pyopenjtalk worker server terminated\n"
          ]
        }
      ],
      "source": [
        "# 学習結果を試す・マージ・スタイル分けはこちらから\n",
        "!python app.py --share"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "p6KdkoOZ6NxK"
      },
      "outputs": [],
      "source": [
        "# ONNX変換は、変換したいsafetensorsファイルを指定してこのセルを実行してください。\n",
        "!python convert_onnx.py --model \"Data/your_model/your_model_e100_s10000.safetensors\""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "901457cf",
        "outputId": "a34f5d00-70c7-4383-8627-cb969f2e6a9a"
      },
      "source": [
        "!cat /content/Style-Bert-VITS2/transcribe.py"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "import argparse\r\n",
            "import sys\r\n",
            "from pathlib import Path\r\n",
            "from typing import Any, Optional\r\n",
            "\r\n",
            "from torch.utils.data import Dataset\r\n",
            "from tqdm import tqdm\r\n",
            "\r\n",
            "from config import get_path_config\r\n",
            "from style_bert_vits2.constants import Languages\r\n",
            "from style_bert_vits2.logging import logger\r\n",
            "from style_bert_vits2.utils.stdout_wrapper import SAFE_STDOUT\r\n",
            "\r\n",
            "\r\n",
            "# faster-whisperは並列処理しても速度が向上しないので、単一モデルでループ処理する\r\n",
            "def transcribe_with_faster_whisper(\r\n",
            "    model: \"WhisperModel\",\r\n",
            "    audio_file: Path,\r\n",
            "    initial_prompt: Optional[str] = None,\r\n",
            "    language: str = \"ja\",\r\n",
            "    num_beams: int = 1,\r\n",
            "    no_repeat_ngram_size: int = 10,\r\n",
            "):\r\n",
            "    segments, _ = model.transcribe(\r\n",
            "        str(audio_file),\r\n",
            "        beam_size=num_beams,\r\n",
            "        language=language,\r\n",
            "        initial_prompt=initial_prompt,\r\n",
            "        no_repeat_ngram_size=no_repeat_ngram_size,\r\n",
            "    )\r\n",
            "    texts = [segment.text for segment in segments]\r\n",
            "    return \"\".join(texts)\r\n",
            "\r\n",
            "\r\n",
            "# HF pipelineで進捗表示をするために必要なDatasetクラス\r\n",
            "class StrListDataset(Dataset[str]):\r\n",
            "    def __init__(self, original_list: list[str]) -> None:\r\n",
            "        self.original_list = original_list\r\n",
            "\r\n",
            "    def __len__(self) -> int:\r\n",
            "        return len(self.original_list)\r\n",
            "\r\n",
            "    def __getitem__(self, i: int) -> str:\r\n",
            "        return self.original_list[i]\r\n",
            "\r\n",
            "\r\n",
            "# HFのWhisperはファイルリストを与えるとバッチ処理ができて速い\r\n",
            "def transcribe_files_with_hf_whisper(\r\n",
            "    audio_files: list[Path],\r\n",
            "    model_id: str,\r\n",
            "    output_file: Path,\r\n",
            "    initial_prompt: Optional[str] = None,\r\n",
            "    language: str = \"ja\",\r\n",
            "    batch_size: int = 16,\r\n",
            "    num_beams: int = 1,\r\n",
            "    no_repeat_ngram_size: int = 10,\r\n",
            "    device: str = \"cuda\",\r\n",
            "    pbar: Optional[tqdm] = None,\r\n",
            ") -> list[str]:\r\n",
            "    import torch\r\n",
            "    from transformers import WhisperProcessor, pipeline\r\n",
            "\r\n",
            "    processor: WhisperProcessor = WhisperProcessor.from_pretrained(model_id)\r\n",
            "    generate_kwargs: dict[str, Any] = {\r\n",
            "        \"language\": language,\r\n",
            "        \"do_sample\": False,\r\n",
            "        \"num_beams\": num_beams,\r\n",
            "        \"no_repeat_ngram_size\": no_repeat_ngram_size,\r\n",
            "    }\r\n",
            "    logger.info(f\"generate_kwargs: {generate_kwargs}, loading pipeline...\")\r\n",
            "    pipe = pipeline(\r\n",
            "        model=model_id,\r\n",
            "        max_new_tokens=128,\r\n",
            "        chunk_length_s=30,\r\n",
            "        batch_size=batch_size,\r\n",
            "        torch_dtype=torch.float16 if device == \"cuda\" else torch.float32,\r\n",
            "        device=device,\r\n",
            "        trust_remote_code=True,\r\n",
            "        # generate_kwargs=generate_kwargs,\r\n",
            "    )\r\n",
            "    logger.info(\"Loaded pipeline\")\r\n",
            "    if initial_prompt is not None:\r\n",
            "        prompt_ids: torch.Tensor = pipe.tokenizer.get_prompt_ids(\r\n",
            "            initial_prompt, return_tensors=\"pt\"\r\n",
            "        ).to(device)\r\n",
            "        generate_kwargs[\"prompt_ids\"] = prompt_ids\r\n",
            "\r\n",
            "    dataset = StrListDataset([str(f) for f in audio_files])\r\n",
            "\r\n",
            "    results: list[str] = []\r\n",
            "    for whisper_result, file in zip(\r\n",
            "        pipe(dataset, generate_kwargs=generate_kwargs), audio_files\r\n",
            "    ):\r\n",
            "        text: str = whisper_result[\"text\"]\r\n",
            "        # なぜかテキストの最初に\" {initial_prompt}\"が入るので、文字の最初からこれを削除する\r\n",
            "        # cf. https://github.com/huggingface/transformers/issues/27594\r\n",
            "        if text.startswith(f\" {initial_prompt}\"):\r\n",
            "            text = text[len(f\" {initial_prompt}\") :]\r\n",
            "        # with open(output_file, \"w\", encoding=\"utf-8\") as f:\r\n",
            "        #     for wav_file, text in zip(wav_files, results):\r\n",
            "        #         wav_rel_path = wav_file.relative_to(input_dir)\r\n",
            "        #         f.write(f\"{wav_rel_path}|{model_name}|{language_id}|{text}\\n\")\r\n",
            "        with open(output_file, \"a\", encoding=\"utf-8\") as f:\r\n",
            "            wav_rel_path = file.relative_to(input_dir)\r\n",
            "            f.write(f\"{wav_rel_path}|{model_name}|{language_id}|{text}\\n\")\r\n",
            "        results.append(text)\r\n",
            "        if pbar is not None:\r\n",
            "            pbar.update(1)\r\n",
            "\r\n",
            "    if pbar is not None:\r\n",
            "        pbar.close()\r\n",
            "\r\n",
            "    return results\r\n",
            "\r\n",
            "\r\n",
            "if __name__ == \"__main__\":\r\n",
            "    parser = argparse.ArgumentParser()\r\n",
            "    parser.add_argument(\"--model_name\", type=str, required=True)\r\n",
            "    parser.add_argument(\r\n",
            "        \"--initial_prompt\",\r\n",
            "        type=str,\r\n",
            "        default=\"こんにちは。元気、ですかー？ふふっ、私は……ちゃんと元気だよ！\",\r\n",
            "    )\r\n",
            "    parser.add_argument(\r\n",
            "        \"--language\", type=str, default=\"ja\", choices=[\"ja\", \"en\", \"zh\"]\r\n",
            "    )\r\n",
            "    parser.add_argument(\"--model\", type=str, default=\"large-v3\")\r\n",
            "    parser.add_argument(\"--device\", type=str, default=\"cuda\")\r\n",
            "    parser.add_argument(\"--compute_type\", type=str, default=\"bfloat16\")\r\n",
            "    parser.add_argument(\"--use_hf_whisper\", action=\"store_true\")\r\n",
            "    parser.add_argument(\"--hf_repo_id\", type=str, default=\"\")\r\n",
            "    parser.add_argument(\"--batch_size\", type=int, default=16)\r\n",
            "    parser.add_argument(\"--num_beams\", type=int, default=1)\r\n",
            "    parser.add_argument(\"--no_repeat_ngram_size\", type=int, default=10)\r\n",
            "    args = parser.parse_args()\r\n",
            "\r\n",
            "    path_config = get_path_config()\r\n",
            "    dataset_root = path_config.dataset_root\r\n",
            "\r\n",
            "    model_name = str(args.model_name)\r\n",
            "\r\n",
            "    input_dir = dataset_root / model_name / \"raw\"\r\n",
            "    output_file = dataset_root / model_name / \"esd.list\"\r\n",
            "    initial_prompt: str = args.initial_prompt\r\n",
            "    initial_prompt = initial_prompt.strip('\"')\r\n",
            "    language: str = args.language\r\n",
            "    device: str = args.device\r\n",
            "    compute_type: str = args.compute_type\r\n",
            "    batch_size: int = args.batch_size\r\n",
            "    num_beams: int = args.num_beams\r\n",
            "    no_repeat_ngram_size: int = args.no_repeat_ngram_size\r\n",
            "\r\n",
            "    output_file.parent.mkdir(parents=True, exist_ok=True)\r\n",
            "\r\n",
            "    wav_files = [f for f in input_dir.rglob(\"*.wav\") if f.is_file()]\r\n",
            "    wav_files = sorted(wav_files, key=lambda x: str(x))\r\n",
            "    logger.info(f\"Found {len(wav_files)} WAV files\")\r\n",
            "    if len(wav_files) == 0:\r\n",
            "        logger.warning(f\"No WAV files found in {input_dir}\")\r\n",
            "        sys.exit(1)\r\n",
            "\r\n",
            "    if output_file.exists():\r\n",
            "        logger.warning(f\"{output_file} exists, backing up to {output_file}.bak\")\r\n",
            "        backup_path = output_file.with_name(output_file.name + \".bak\")\r\n",
            "        if backup_path.exists():\r\n",
            "            logger.warning(f\"{output_file}.bak exists, deleting...\")\r\n",
            "            backup_path.unlink()\r\n",
            "        output_file.rename(backup_path)\r\n",
            "\r\n",
            "    if language == \"ja\":\r\n",
            "        language_id = Languages.JP.value\r\n",
            "    elif language == \"en\":\r\n",
            "        language_id = Languages.EN.value\r\n",
            "    elif language == \"zh\":\r\n",
            "        language_id = Languages.ZH.value\r\n",
            "    else:\r\n",
            "        raise ValueError(f\"{language} is not supported.\")\r\n",
            "\r\n",
            "    if not args.use_hf_whisper:\r\n",
            "        from faster_whisper import WhisperModel\r\n",
            "\r\n",
            "        logger.info(\r\n",
            "            f\"Loading faster-whisper model ({args.model}) with compute_type={compute_type}\"\r\n",
            "        )\r\n",
            "        try:\r\n",
            "            model = WhisperModel(args.model, device=device, compute_type=compute_type)\r\n",
            "        except ValueError as e:\r\n",
            "            logger.warning(f\"Failed to load model, so use `auto` compute_type: {e}\")\r\n",
            "            model = WhisperModel(args.model, device=device)\r\n",
            "        for wav_file in tqdm(wav_files, file=SAFE_STDOUT, dynamic_ncols=True):\r\n",
            "            text = transcribe_with_faster_whisper(\r\n",
            "                model=model,\r\n",
            "                audio_file=wav_file,\r\n",
            "                initial_prompt=initial_prompt,\r\n",
            "                language=language,\r\n",
            "                num_beams=num_beams,\r\n",
            "                no_repeat_ngram_size=no_repeat_ngram_size,\r\n",
            "            )\r\n",
            "            wav_rel_path = wav_file.relative_to(input_dir)\r\n",
            "            with open(output_file, \"a\", encoding=\"utf-8\") as f:\r\n",
            "                f.write(f\"{wav_rel_path}|{model_name}|{language_id}|{text}\\n\")\r\n",
            "    else:\r\n",
            "        model_id = args.hf_repo_id\r\n",
            "        logger.info(f\"Loading HF Whisper model ({model_id})\")\r\n",
            "        pbar = tqdm(total=len(wav_files), file=SAFE_STDOUT, dynamic_ncols=True)\r\n",
            "        results = transcribe_files_with_hf_whisper(\r\n",
            "            audio_files=wav_files,\r\n",
            "            model_id=model_id,\r\n",
            "            initial_prompt=initial_prompt,\r\n",
            "            language=language,\r\n",
            "            batch_size=batch_size,\r\n",
            "            num_beams=num_beams,\r\n",
            "            no_repeat_ngram_size=no_repeat_ngram_size,\r\n",
            "            device=device,\r\n",
            "            pbar=pbar,\r\n",
            "            output_file=output_file,\r\n",
            "        )\r\n",
            "        # with open(output_file, \"w\", encoding=\"utf-8\") as f:\r\n",
            "        #     for wav_file, text in zip(wav_files, results):\r\n",
            "        #         wav_rel_path = wav_file.relative_to(input_dir)\r\n",
            "        #         f.write(f\"{wav_rel_path}|{model_name}|{language_id}|{text}\\n\")\r\n",
            "\r\n",
            "    sys.exit(0)\r\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "586c81f5",
        "outputId": "72dffa20-ac7b-4deb-df53-ed8d2b199c55"
      },
      "source": [
        "import os\n",
        "\n",
        "dataset_root = \"/content/drive/MyDrive/Style-Bert-VITS2/Data\"\n",
        "model_name = \"your_model_name\"\n",
        "esd_list_path = os.path.join(dataset_root, model_name, \"esd.list\")\n",
        "\n",
        "if os.path.exists(esd_list_path):\n",
        "    print(f\"'{esd_list_path}' が見つかりました。\")\n",
        "else:\n",
        "    print(f\"'{esd_list_path}' は見つかりませんでした。\")\n",
        "    print(\"データ準備のステップが正しく完了しているか再度確認してください。\")"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "'/content/drive/MyDrive/Style-Bert-VITS2/Data/your_model_name/esd.list' は見つかりませんでした。\n",
            "データ準備のステップが正しく完了しているか再度確認してください。\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "af788c64",
        "outputId": "8eebf4a0-c8e5-4fd5-ac68-16c15131b55c"
      },
      "source": [
        "import os\n",
        "from pathlib import Path\n",
        "from style_bert_vits2.logging import logger\n",
        "from transformers import WhisperProcessor, pipeline\n",
        "import torch\n",
        "from torch.utils.data import Dataset\n",
        "from tqdm import tqdm\n",
        "from typing import Optional, Any\n",
        "\n",
        "# HF pipelineで進捗表示をするために必要なDatasetクラス\n",
        "class StrListDataset(Dataset[str]):\n",
        "    def __init__(self, original_list: list[str]) -> None:\n",
        "        self.original_list = original_list\n",
        "\n",
        "    def __len__(self) -> int:\n",
        "        return len(self.original_list)\n",
        "\n",
        "    def __getitem__(self, i: int) -> str:\n",
        "        return self.original_list[i]\n",
        "\n",
        "# HFのWhisperはファイルリストを与えるとバッチ処理ができて速い\n",
        "def transcribe_files_with_hf_whisper(\n",
        "    audio_files: list[Path],\n",
        "    model_id: str,\n",
        "    output_file: Path,\n",
        "    initial_prompt: Optional[str] = None,\n",
        "    language: str = \"ja\",\n",
        "    batch_size: int = 16,\n",
        "    num_beams: int = 1,\n",
        "    no_repeat_ngram_size: int = 10,\n",
        "    device: str = \"cuda\",\n",
        "    pbar: Optional[tqdm] = None,\n",
        ") -> list[str]:\n",
        "    logger.info(f\"Loading HF Whisper model ({model_id})\")\n",
        "\n",
        "    processor: WhisperProcessor = WhisperProcessor.from_pretrained(model_id)\n",
        "    generate_kwargs: dict[str, Any] = {\n",
        "        \"language\": language,\n",
        "        \"do_sample\": False,\n",
        "        \"num_beams\": num_beams, # Corrected syntax: num_beams=...\n",
        "        \"no_repeat_ngram_size\": no_repeat_ngram_size, # Corrected syntax: no_repeat_ngram_size=...\n",
        "    }\n",
        "    logger.info(f\"generate_kwargs: {generate_kwargs}, loading pipeline...\")\n",
        "    pipe = pipeline(\n",
        "        model=model_id,\n",
        "        max_new_tokens=128,\n",
        "        chunk_length_s=30,\n",
        "        batch_size=batch_size,\n",
        "        torch_dtype=torch.float16 if device == \"cuda\" else torch.float32,\n",
        "        device=device,\n",
        "        trust_remote_code=True,\n",
        "        # generate_kwargs=generate_kwargs, # NOTE: generate_kwargs is passed below in the pipe call\n",
        "    )\n",
        "    logger.info(\"Loaded pipeline\")\n",
        "    if initial_prompt is not None:\n",
        "        prompt_ids: torch.Tensor = pipe.tokenizer.get_prompt_ids(\n",
        "            initial_prompt, return_tensors=\"pt\"\n",
        "        ).to(device)\n",
        "        generate_kwargs[\"prompt_ids\"] = prompt_ids\n",
        "\n",
        "\n",
        "    dataset = StrListDataset([str(f) for f in audio_files])\n",
        "\n",
        "    results: list[str] = []\n",
        "    for whisper_result, file in zip(\n",
        "        pipe(dataset, generate_kwargs=generate_kwargs), audio_files\n",
        "    ):\n",
        "        text: str = whisper_result[\"text\"]\n",
        "        # なぜかテキストの最初に\" {initial_prompt}\"が入るので、文字の最初からこれを削除する\n",
        "        # cf. https://github.com/huggingface/transformers/issues/27594\n",
        "        if initial_prompt is not None and text.startswith(f\" {initial_prompt}\"):\n",
        "            text = text[len(f\" {initial_prompt}\") :]\n",
        "\n",
        "        # Write to esd.list\n",
        "        try:\n",
        "            # Use relative path from dataset_root/model_name/raw/\n",
        "            input_dir = output_file.parent / \"raw\"\n",
        "            wav_rel_path = file.relative_to(input_dir)\n",
        "            language_id = \"JP\" # Assuming JP based on previous setup, needs adjustment if language is variable\n",
        "\n",
        "            with open(output_file, \"a\", encoding=\"utf-8\") as f:\n",
        "                f.write(f\"{wav_rel_path}|{model_name}|{language_id}|{text.strip()}\\n\") # Added strip() to remove leading/trailing whitespace\n",
        "            results.append(text)\n",
        "            if pbar is not None:\n",
        "                pbar.update(1)\n",
        "        except Exception as e:\n",
        "            logger.error(f\"Error writing to esd.list for file {file}: {e}\")\n",
        "            # Continue processing other files even if one fails to write\n",
        "\n",
        "\n",
        "    if pbar is not None:\n",
        "        pbar.close()\n",
        "\n",
        "    return results\n",
        "\n",
        "# --- Execution starts here ---\n",
        "dataset_root = Path(\"/content/drive/MyDrive/Style-Bert-VITS2/Data\")\n",
        "model_name = \"your_model_name\" # Make sure this matches your model name\n",
        "input_dir = dataset_root / model_name / \"raw\"\n",
        "output_file = dataset_root / model_name / \"esd.list\"\n",
        "initial_prompt = \"こんにちは。元気、ですかー？ふふっ、私は……ちゃんと元気だよ！\" # Make sure this matches your initial prompt\n",
        "language = \"ja\"\n",
        "model_id = \"openai/whisper-large-v3\" # Use the working model ID\n",
        "batch_size = 16\n",
        "num_beams = 1\n",
        "no_repeat_ngram_size = 10\n",
        "device = \"cuda\"\n",
        "\n",
        "# Ensure output directory exists\n",
        "output_file.parent.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "# Find WAV files in the raw directory\n",
        "wav_files = [f for f in input_dir.rglob(\"*.wav\") if f.is_file()]\n",
        "wav_files = sorted(wav_files, key=lambda x: str(x))\n",
        "\n",
        "if len(wav_files) == 0:\n",
        "    print(f\"No WAV files found in {input_dir}. Please ensure slice.py ran correctly.\")\n",
        "else:\n",
        "    print(f\"Found {len(wav_files)} WAV files in {input_dir}.\")\n",
        "    # Delete esd.list if it exists to avoid appending to old data\n",
        "    if output_file.exists():\n",
        "        logger.warning(f\"{output_file} exists, backing up to {output_file}.bak\")\n",
        "        backup_path = output_file.with_name(output_file.name + \".bak\")\n",
        "        if backup_path.exists():\n",
        "             backup_path.unlink() # Delete existing backup\n",
        "        output_file.rename(backup_path)\n",
        "        logger.info(f\"Backed up existing {output_file} to {backup_path}\")\n",
        "\n",
        "\n",
        "    pbar = tqdm(total=len(wav_files), dynamic_ncols=True)\n",
        "\n",
        "    transcribe_files_with_hf_whisper(\n",
        "        audio_files=wav_files,\n",
        "        model_id=model_id,\n",
        "        output_file=output_file,\n",
        "        initial_prompt=initial_prompt,\n",
        "        language=language,\n",
        "        batch_size=batch_size,\n",
        "        num_beams=num_beams,\n",
        "        no_repeat_ngram_size=no_repeat_ngram_size,\n",
        "        device=device,\n",
        "        pbar=pbar,\n",
        "    )\n",
        "\n",
        "    print(f\"\\nTranscription process completed. Checking if {output_file} was created.\")\n",
        "\n",
        "    # Final check after transcription\n",
        "    if output_file.exists():\n",
        "        print(f\"'{output_file}' が見つかりました。\")\n",
        "        # Optional: Display content of esd.list\n",
        "        # with open(output_file, 'r', encoding='utf-8') as f:\n",
        "        #     print(\"\\nContent of esd.list:\")\n",
        "        #     print(f.read())\n",
        "    else:\n",
        "        print(f\"'{output_file}' は見つかりませんでした。\")\n",
        "        print(\"書き起こしファイル生成に失敗した可能性があります。\")"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 1 WAV files in /content/drive/MyDrive/Style-Bert-VITS2/Data/your_model_name/raw.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r  0%|          | 0/1 [00:00<?, ?it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "08-31 07:58:37 |  INFO  | ipython-input-4168086732.py:34 | Loading HF Whisper model (openai/whisper-large-v3)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "08-31 07:58:39 |  INFO  | ipython-input-4168086732.py:43 | generate_kwargs: {'language': 'ja', 'do_sample': False, 'num_beams': 1, 'no_repeat_ngram_size': 10}, loading pipeline...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Device set to use cuda\n",
            "Using `chunk_length_s` is very experimental with seq2seq models. The results will not necessarily be entirely accurate and will have caveats. More information: https://github.com/huggingface/transformers/pull/20104. Ignore this warning with pipeline(..., ignore_warning=True). To use Whisper for long-form transcription, use rather the model's `generate` method directly as the model relies on it's own chunking mechanism (cf. Whisper original paper, section 3.8. Long-form Transcription).\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "08-31 07:58:49 |  INFO  | ipython-input-4168086732.py:54 | Loaded pipeline\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1/1 [00:15<00:00, 15.02s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Transcription process completed. Checking if /content/drive/MyDrive/Style-Bert-VITS2/Data/your_model_name/esd.list was created.\n",
            "'/content/drive/MyDrive/Style-Bert-VITS2/Data/your_model_name/esd.list' が見つかりました。\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d00ff91c",
        "outputId": "4ef1a621-5fab-48de-f0a7-0e5a1ad26f16"
      },
      "source": [
        "import os\n",
        "\n",
        "model_assets_path = \"/content/drive/MyDrive/Style-Bert-VITS2/model_assets/your_model_name\"\n",
        "model_file_name = \"your_model_name_e100_s100.safetensors\"\n",
        "model_file_path = os.path.join(model_assets_path, model_file_name)\n",
        "\n",
        "if os.path.exists(model_file_path):\n",
        "    print(f\"'{model_file_path}' が見つかりました。\")\n",
        "else:\n",
        "    print(f\"'{model_file_path}' は見つかりませんでした。\")\n",
        "    print(\"学習が正常に完了し、モデルファイルが保存されているか確認してください。\")"
      ],
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "'/content/drive/MyDrive/Style-Bert-VITS2/model_assets/your_model_name/your_model_name_e100_s100.safetensors' が見つかりました。\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 211
        },
        "id": "caa4af65",
        "outputId": "2a8ab5cb-0738-429e-f483-835966bfaa42"
      },
      "source": [
        "import yaml\n",
        "\n",
        "config_path = \"/content/Style-Bert-VITS2/config.yml\"\n",
        "\n",
        "with open(config_path, \"r\", encoding=\"utf-8\") as f:\n",
        "    config_data = yaml.safe_load(f)\n",
        "\n",
        "# Update the model path for the webui section\n",
        "# Assuming the model is saved as your_model_name/your_model_name_e100_s100.safetensors\n",
        "# relative to the assets_root\n",
        "model_name = config_data.get(\"model_name\", \"your_model_name\") # Get model_name from config, default to your_model_name\n",
        "model_file_name = f\"{model_name}_e100_s100.safetensors\" # Construct the expected file name\n",
        "config_data[\"webui\"][\"model\"] = f\"{model_name}/{model_file_name}\" # Set the path relative to assets_root\n",
        "\n",
        "\n",
        "with open(config_path, \"w\", encoding=\"utf-8\") as f:\n",
        "    yaml.dump(config_data, f, allow_unicode=True)\n",
        "\n",
        "print(f\"'{config_path}' を更新しました。 webui.model のパスを '{config_data['webui']['model']}' に設定しました。\")"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "[Errno 2] No such file or directory: '/content/Style-Bert-VITS2/config.yml'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-4041295857.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mconfig_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"/content/Style-Bert-VITS2/config.yml\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"r\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"utf-8\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m     \u001b[0mconfig_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0myaml\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msafe_load\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/content/Style-Bert-VITS2/config.yml'"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "411d7741"
      },
      "source": [
        "`config.yml` の更新が完了したら、再度以下のコードセルを実行して `app.py` を起動してください。今度は学習したモデルが読み込まれるはずです。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d0b72fc8",
        "outputId": "94fceaea-40ca-4eef-808d-3d509b8ca872"
      },
      "source": [
        "# 学習結果を試す・マージ・スタイル分けはこちらから\n",
        "!python app.py --share"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "python3: can't open file '/content/app.py': [Errno 2] No such file or directory\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.11"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}